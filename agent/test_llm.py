from llama_utils import ask_local_llm

print(ask_local_llm("Can you show me the temperature readings?"))
print(ask_local_llm("Are there any anomalies today?"))
print(ask_local_llm("Whatâ€™s the system health status?"))
